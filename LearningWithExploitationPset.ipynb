{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGNMAUO9-h8y"
   },
   "source": [
    "# Team 5 - Learning Models by Exploiting Structure and Fundamental Knowledge\n",
    "\n",
    "\n",
    "1. [Introduction](#robot-world)\n",
    "    1. [Computing Gradient of a Function - Mini Tutorial](#gradient-tutorial)\n",
    "    2. [Loss Functions](#loss-functions)\n",
    "    3. [Worked Example](#example)\n",
    "2. [Problem A: Temporal Cohesion Prior (10 points)](#partA)\n",
    "3. [Problem B: Causality Prior (20 points)](#partB)\n",
    "4. [Problem C: Repeatability Prior (20 points)](#partC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vpeUoawLVr4"
   },
   "source": [
    "Stuff to add\n",
    "\n",
    "*   Background on how to take a gradient\n",
    "*   tell a story to justify why we are implementing these loss functions\n",
    "*   ground each function in what part of the state representation we want to learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a id='robot-world'/>\n",
    "\n",
    "We have deployed our robot in a simple 2D world, and now the robot needs our help to learn the state representation of its world. The world is a 10x10 grid with our robot occupying one of the cells. The robot knows what actions it can take in this envrionment and also gets rewards. The robot gets more rewards the further away it travels from (0,0) on the grid.\n",
    "\n",
    "As students of Cognitive Robotics course, the robot has come to you to help improve its learning model so that he can have an accurate state representation of where it is in the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradient of a Function - Mini Tutorial <a id='gradient-tutorial'/>\n",
    "\n",
    "The robot has found some material online to help you refresh your memory on how to take gradient of a function.\n",
    "Let's take a look at some basic simple examples:\n",
    "\n",
    "$f(x) = x^{4} + x + 1$. What is the $\\nabla f(x)$ at $x = 1$ ?\n",
    "\n",
    "We take the partial derivative with respect to $x$, $f_{x}$ as\n",
    "\n",
    "$f_{x} = 4 * x^{3} + 1$\n",
    "\n",
    "Then, $\\nabla f(x = 1) = 4 * (1)^{3} + 1 = 5$\n",
    "\n",
    "$f(x,y) = e^{2x} \\sin(3y)$. What is the $\\nabla f(x,y)$ at $x = 2, y = -2$ ?\n",
    "\n",
    "We can formulate the gradient of the function $\\nabla f(x,y) = <f_{x}, f_{y}>$ where $f_{x}$ and $f_{y}$ are partial derivates with respect to $x$ and $y$ respectively.\n",
    "\n",
    "$f_{x} = 2 * e^{2x} \\sin(3y)$\n",
    "\n",
    "$f_{y} = 3 * e^{2x} \\cos(3y)$\n",
    "\n",
    "Finally, $\\nabla f(2,-2) = <2 * e^{2x} \\sin(3y),  3 * e^{2x} \\cos(3y)>$\n",
    "\n",
    "$\\nabla f(2,-2) = <2 * e^{4} \\sin(-6),  3 * e^{4} \\cos(-6)>$\n",
    "\n",
    "$\\nabla f(2,-2) = <37.51,  157.27>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xg0E5IXNw_B"
   },
   "outputs": [],
   "source": [
    "from checker import check_solution\n",
    "import solutions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBEiE84H-oqZ"
   },
   "source": [
    "## Loss Functions <a id='loss-functions'/>\n",
    "\n",
    "In lecture, we stepped through multiple components of a loss function that is used to learn physics-based state representations. However, we do not use the loss function directly during learning, but rather the gradient of the loss function to perform gradient descent. In this problem, you will implement the gradient of each component of the loss function.\n",
    "\n",
    "The incoming data will be formatted as an array of Data_Frame objects which are defined in utils_pset. At a high level, these objects have:\n",
    "\n",
    "\n",
    "\n",
    "*   `action`: an integer indicating the action taken at the given timestep (up, down, left right)\n",
    "*   `reward`: the reward received at the given timestep for moving away from a position.\n",
    "*   `image`: a flattened array of the pixel values of the image taken of the agent (the robot on the grid)\n",
    "\n",
    "Using the incoming data as well as the matrix used to transform image vectors to coordinates, you will step through and implement the loss function defined in lecture that biases state representations toward following Newton's laws of physics. Each section will step you through implementing a different component of the loss function: temporal cohesion, proportionality, causality, repeatability. These are the physical prior of the environment for this representation.\n",
    "\n",
    "The mapping matrix is the matrix we are learning to transform high dimensional image vectors to low dimensional state representations. Wherever we use the variable $s_t$ to represent a state, this can be thought of as $s_t = m v_t$, where $m$ is the mapping matrix that we are learning, and $v_t$ is the image vector taken of the world at time $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pset Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1Asfz4UZlpJ"
   },
   "source": [
    "\n",
    "## Worked Example <a id='example'/>\n",
    "\n",
    "First, we want our agent to learn state representations that follow the rule that F=ma. This means we need to implement the Proportionality Prior Gradient.\n",
    "\n",
    "The proportionality prior loss is defined as $E[(||\\triangle s_{t_2}|| - ||\\triangle s_{t_1}||)^2 | a_{t_1} = a_{t_2}]$, for two time steps $t_1$ and $t_2$. \n",
    "\n",
    "We have implemented this for you to give you an example of how to implement the remaining loss function components. We have broken the gradient into two components â€” an outer loop and an innter loop. The outer loop (executed in `proportionality_prior_gradient`) corresponds to the expectation portion of the function, where we want to find time steps where the agent took the same action. \n",
    "\n",
    "We then use a helper function, `proportional_loss_gradient`, to compute the gradient of $||\\triangle s_{t_2}|| - ||\\triangle s_{t_1}||)^2$. Now remember, to compute this gradient, it is useful to think of the function less in terms of the state representation themselves $s$, but rather in terms of the image vectors ($v_{t_1}, v_{t_1+1}, v_{t_2}, v_{t_2+1}$) and the mapping matrix ($m$). Which would give you:\n",
    "\n",
    "$\\frac{d}{dm} (||m v_{t_1 +1} - m v_{t_1}|| - ||m v_{t_2 +1} - m v_{t_2}||)^2$\n",
    "\n",
    "Which is important to think about when computing gradients because your gradients are always with respect to $m$, which is concealed inside the state representations. This derivative is computer below by iterating over each element of $m$ and computing the gradient of it with respect to the rest of the lost function, and then we return the entire gradient of $m$. \n",
    "\n",
    "You can see the gradient computation is grounded, not in the individual value of the state representations, but rather in the delta between state representations, and is focused on measuring the difference between those deltas. In implementing the gradients for the remaining functions, we hope you can refine your mathematical intuition behind the formulation of these loss function components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5Mu56Hj_T8v"
   },
   "outputs": [],
   "source": [
    "def proportionality_prior_gradient(batch, mapping):\n",
    "  \"\"\"\n",
    "  :param batch an array of Data_Frame objects\n",
    "  :param mapping the current matrix that maps flattened image vectors to coordinates\n",
    "  :return the gradient of the proportionality prior loss (should be the same dimensions as the mapping matrix)\n",
    "  \"\"\"\n",
    "\n",
    "  time_steps = len(batch)\n",
    "  total_loss_grad = np.zeros(mapping.shape)\n",
    "  for i in range(0, time_steps - 1):\n",
    "      for j in range(i+1,time_steps - 1):\n",
    "          if batch[i].action == batch[j].action:\n",
    "              loss_grad = proportional_loss_gradient(batch[i].image, batch[i + 1].image, batch[j].image, batch[j+1].image, mapping)\n",
    "              total_loss_grad += loss_grad.reshape(mapping.shape)\n",
    "\n",
    "  return total_loss_grad / (time_steps - 1)\n",
    "\n",
    "\n",
    "def proportional_loss_gradient(s1, s2, s3, s4, mapping):\n",
    "    dims = mapping.shape\n",
    "    output = np.zeros(dims)\n",
    "    delta1 = mapping@s2 - mapping@s1\n",
    "    delta2 = mapping@s4 - mapping@s3\n",
    "    outest = (np.linalg.norm(delta2) - np.linalg.norm(delta1))*2\n",
    "\n",
    "    for i in range(0, dims[0]):\n",
    "\n",
    "        outer_denom_1 = delta1[i]*np.linalg.norm(delta1)\n",
    "        outer_denom_2 = delta2[i]*np.linalg.norm(delta2)\n",
    "        for j in range(0, dims[1]):\n",
    "            frac_1 = (delta1[i]**2)*(s1[j]- s2[j])/outer_denom_1\n",
    "            frac_2 = (delta2[i]**2)*(s3[j] - s4[j])/outer_denom_2\n",
    "            output[i,j] = -(frac_2 - frac_1)*outest\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxXfa-K6-sM3"
   },
   "source": [
    "## Problem A: Temporal Cohesion Prior <a id='partA'/>\n",
    "\n",
    "The temporal cohesion loss represents Newton's Law of Inertia by penalizing temporally adjacent timeframes for being too different from each other.\n",
    "\n",
    "In lecture, we defined the temporal cohesion loss as $E[||\\triangle s_t||^2]$, where $s_t$ is the state representation at time $t$.\n",
    "Use the function skeleton below to implement the gradient of the temporal cohesion loss.\n",
    "\n",
    "Hint: You will want to take the derivative of $E[||\\triangle s_t||^2]$ with respect to the `mapping` matrix, and implement that as part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZQ1yGFu-35S"
   },
   "outputs": [],
   "source": [
    "def temporal_cohesion_gradient(batch, mapping):\n",
    "  \"\"\"\n",
    "  :param batch an array of Data_Frame objects\n",
    "  :param mapping the current matrix that maps flattened image vectors to coordinates\n",
    "  :return the gradient of the temporal cohesion loss (should be the same dimensions as the mapping matrix)\n",
    "  \"\"\"\n",
    "\n",
    "  raise NotImplementedError(\"finish this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GfASWz6MmkB",
    "outputId": "e611466b-f5f4-4371-df72-bafac744de74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "mapping = np.arange(200).reshape((2, 100))\n",
    "print(check_solution(temporal_cohesion_gradient, solutions.temporal_cohesion_sol, mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W36sTMP_Ubd"
   },
   "source": [
    "## Problem B: Causality Prior <a id='partB'/>\n",
    "\n",
    "The causality prior loss represents Newton's Third Law of equal and opposite reactions by penalizing same actions with different rewards having similar state representations.\n",
    "\n",
    "In lecture, we defined the proportionality prior loss between two timesteps $t_1$ and $t_2$ as $E[e^{-|| s_{t_2} -  s_{t_1}||} | a_{t_1} = a_{t_2} \\wedge r_{t_1} \\neq r_{t_2}]$.\n",
    "Use the function skeleton below to implement the gradient of the causality prior loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xdK-ta9_YiH"
   },
   "outputs": [],
   "source": [
    "def causality_prior_gradient(batch, mapping):\n",
    "  \"\"\"\n",
    "  :param batch an array of Data_Frame objects\n",
    "  :param mapping the current matrix that maps flattened image vectors to coordinates\n",
    "  :return the gradient of the causality prior loss (should be the same dimensions as the mapping matrix)\n",
    "  \"\"\"\n",
    "\n",
    "  #TODO: Your code here\n",
    "\n",
    "\n",
    "  raise NotImplementedError(\"finish this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnmRcK4e_Y0C"
   },
   "source": [
    "## Problem C: Repeatability Prior <a id='partC'/>\n",
    "\n",
    "The repeatability prior loss represents Newton's Third Law of equal and opposite reactions by penalizing same actions taken from similar positions for transitioning to different positions.\n",
    "\n",
    "In lecture, we defined the proportionality prior loss between two timesteps $t_1$ and $t_2$ as $E[e^{-|| s_{t_2} -  s_{t_1}||}||\\triangle s_{t_2} - \\triangle s_{t_1}||^2  | a_{t_1} = a_{t_2}]$.\n",
    "Use the function skeleton below to implement the gradient of the repeatability prior loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1R2jmEdD2si1"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MckklDWe_dD9"
   },
   "outputs": [],
   "source": [
    "def repeatability_prior_gradient(batch, mapping):\n",
    "  \"\"\"\n",
    "  :param batch an array of Data_Frame objects\n",
    "  :param mapping the current matrix that maps flattened image vectors to coordinates\n",
    "  :return the gradient of the repeatability prior loss (should be the same dimensions as the mapping matrix)\n",
    "  \"\"\"\n",
    "\n",
    "  #TODO: Your code here\n",
    "\n",
    "\n",
    "  raise NotImplementedError(\"finish this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE!! Great Work!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
