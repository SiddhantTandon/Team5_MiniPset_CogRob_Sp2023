{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGNMAUO9-h8y"
   },
   "source": [
    "# Team 5 - Learning Models by Exploiting Structure and Fundamental Knowledge\n",
    "\n",
    "# SOLUTIONS NOTEBOOK\n",
    "\n",
    "\n",
    "1. [Introduction](#robot-world)\n",
    "    1. [Computing Gradient of a Function - Mini Tutorial](#gradient-tutorial)\n",
    "    2. [Loss Functions](#loss-functions)\n",
    "    3. [Worked Example](#example)\n",
    "2. [Problem A: Temporal Cohesion Prior (15 points)](#partA)\n",
    "3. [Problem B: Causality Prior (15 points)](#partB)\n",
    "4. [Problem C: Repeatability Prior (20 points)](#partC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction <a id='robot-world'/>\n",
    "\n",
    "We have deployed our robot in a simple 2D world, and now the robot needs our help to learn the state representation of its world. The world is a 10x10 grid with our robot occupying one of the cells. The robot knows what actions it can take in this envrionment and also gets rewards. The robot gets more rewards the further away it travels from (0,0) on the grid.\n",
    "\n",
    "As students of Cognitive Robotics course, the robot has come to you to help improve its learning model so that he can have an accurate state representation of where it is in the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradient of a Function - Mini Tutorial <a id='gradient-tutorial'/>\n",
    "\n",
    "The robot has found some material online to help you refresh your memory on how to take gradient of a function.\n",
    "Let's take a look at some basic simple examples:\n",
    "\n",
    "$f(x) = x^{4} + x + 1$. What is the $\\nabla f(x)$ at $x = 1$ ?\n",
    "\n",
    "We take the partial derivative with respect to $x$, $f_{x}$ as\n",
    "\n",
    "$f_{x} = 4 * x^{3} + 1$\n",
    "\n",
    "Then, $\\nabla f(x = 1) = 4 * (1)^{3} + 1 = 5$\n",
    "\n",
    "$f(x,y) = e^{2x} \\sin(3y)$. What is the $\\nabla f(x,y)$ at $x = 2, y = -2$ ?\n",
    "\n",
    "We can formulate the gradient of the function $\\nabla f(x,y) = <f_{x}, f_{y}>$ where $f_{x}$ and $f_{y}$ are partial derivates with respect to $x$ and $y$ respectively.\n",
    "\n",
    "$f_{x} = 2 * e^{2x} \\sin(3y)$\n",
    "\n",
    "$f_{y} = 3 * e^{2x} \\cos(3y)$\n",
    "\n",
    "Finally, $\\nabla f(2,-2) = <2 * e^{2x} \\sin(3y),  3 * e^{2x} \\cos(3y)>$\n",
    "\n",
    "$\\nabla f(2,-2) = <2 * e^{4} \\sin(-6),  3 * e^{4} \\cos(-6)>$\n",
    "\n",
    "$\\nabla f(2,-2) = <37.51,  157.27>$\n",
    "\n",
    "Here is an [online tutorial](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives) for revising partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_xg0E5IXNw_B",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import checker\n",
    "import numpy as np\n",
    "from utils_pset import Data_Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBEiE84H-oqZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loss Functions <a id='loss-functions'/>\n",
    "\n",
    "In lecture, we stepped through multiple components of a loss function that is used to learn physics-based state representations. However, we do not use the loss function directly during learning, but rather the gradient of the loss function to perform gradient descent. In this problem, you will implement the gradient of each component of the loss function.\n",
    "\n",
    "The incoming data will be formatted as an array of Data_Frame objects which are defined in utils_pset. At a high level, these objects have:\n",
    "\n",
    "Using the incoming data as well as the matrix used to transform image vectors to coordinates, you will step through and implement the loss function defined in lecture that biases state representations toward following Newton's laws of physics. Each section will step you through implementing a different component of the loss function: temporal cohesion, proportionality, causality, repeatability. These are the physical prior of the environment for this representation.\n",
    "\n",
    "The mapping matrix is the matrix we are learning to transform high dimensional image vectors to low dimensional state representations. Wherever we use the variable $s_t$ to represent a state, this can be thought of as $s_t = m v_t$, where $m$ is the mapping matrix that we are learning, and $v_t$ is the image vector taken of the world at time $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "\n",
    "Data will be given to you as an array of Data_Frame objects with the following attributes:\n",
    "\n",
    "* `frame.action`: an integer indicating the action taken at the given timestep (up, down, left right)\n",
    "*  `frame.reward`: the reward received at the given timestep for moving away from a position.\n",
    "*  `frame.image`: a flattened array of the pixel values of the image taken of the agent (the robot on the grid)\n",
    "\n",
    "Additionally, the mapping matrix will be passed as input as an 2x100 matrix, since our state representation\n",
    "dimension is 2, and our image vector dimension is 100 (from the 10x10 image).\n",
    "\n",
    "Remember: you can use `np.linalg.norm` to take the magnitude of a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1Asfz4UZlpJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Worked Example <a id='example'/>\n",
    "\n",
    "First, we want our agent to learn state representations that follow the rule that F=ma. This means we need to implement the Proportionality Prior Gradient.\n",
    "\n",
    "The proportionality prior loss is defined as $E[(||\\triangle s_{t_2}|| - ||\\triangle s_{t_1}||)^2 | a_{t_1} = a_{t_2}]$, for two time steps $t_1$ and $t_2$. \n",
    "\n",
    "We have implemented this for you to give you an example of how to implement the remaining loss function components. We have broken the gradient into two components â€” an outer loop and an innter loop. The outer loop (executed in `proportionality_prior_gradient`) corresponds to the expectation portion of the function, where we want to find time steps where the agent took the same action. \n",
    "\n",
    "We then use a helper function, `proportional_loss_gradient`, to compute the gradient of $||\\triangle s_{t_2}|| - ||\\triangle s_{t_1}||)^2$. Now remember, to compute this gradient, it is useful to think of the function less in terms of the state representation themselves $s$, but rather in terms of the image vectors ($v_{t_1}, v_{t_1+1}, v_{t_2}, v_{t_2+1}$) and the mapping matrix ($m$). Which would give you:\n",
    "\n",
    "$\\frac{d}{dm} (||m v_{t_1 +1} - m v_{t_1}|| - ||m v_{t_2 +1} - m v_{t_2}||)^2$\n",
    "\n",
    "Which is important to think about when computing gradients because your gradients are always with respect to $m$, which is concealed inside the state representations. This derivative is computer below by iterating over each element of $m$ and computing the gradient of it with respect to the rest of the lost function, and then we return the entire gradient of $m$. \n",
    "\n",
    "You can see the gradient computation is grounded, not in the individual value of the state representations, but rather in the delta between state representations, and is focused on measuring the difference between those deltas. In implementing the gradients for the remaining functions, we hope you can refine your mathematical intuition behind the formulation of these loss function components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_5Mu56Hj_T8v"
   },
   "outputs": [],
   "source": [
    "def proportionality_prior_gradient(batch, mapping):\n",
    "    \"\"\"\n",
    "    :param batch an array of Data_Frame objects\n",
    "    :param mapping the current matrix that maps flattened image vectors to coordinates\n",
    "    :return the gradient of the proportionality prior loss (should be the same dimensions as the mapping matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    time_steps = len(batch)\n",
    "    total_loss_grad = np.zeros(mapping.shape)\n",
    "    pairings = 0\n",
    "    for i in range(0, time_steps - 1):\n",
    "      # for each time step\n",
    "      for j in range(i+1,time_steps - 1):\n",
    "          # look at each later time step (to avoid double counting)\n",
    "          if batch[i].action == batch[j].action:\n",
    "              # if we have the same action, calculate the gradient for this\n",
    "              # time step pairing and add to the total\n",
    "              pairings += 1\n",
    "              loss_grad = proportional_loss_gradient(batch[i].image, batch[i + 1].image, batch[j].image, batch[j+1].image, mapping)\n",
    "              total_loss_grad += loss_grad.reshape(mapping.shape)\n",
    "    # return the average over all of the pairings we found\n",
    "    return total_loss_grad / pairings\n",
    "\n",
    "\n",
    "def proportional_loss_gradient(s1, s2, s3, s4, mapping):\n",
    "    \"\"\"\n",
    "    :param s1 image vector at t1\n",
    "    :param s2 image vector at t1+1\n",
    "    :param s3 image vector at t2\n",
    "    :param s4 image vector at t2+1\n",
    "    :return the loss gradient for just the t1/t2 time step pairing\n",
    "    \"\"\"\n",
    "\n",
    "    dims = mapping.shape\n",
    "    output = np.zeros(dims)\n",
    "\n",
    "    delta1 = mapping@s2 - mapping@s1\n",
    "    delta2 = mapping@s4 - mapping@s3\n",
    "\n",
    "    #first layer of chain rule from derivative calculation\n",
    "    outest = (np.linalg.norm(delta2) - np.linalg.norm(delta1))*2\n",
    "\n",
    "    # iterate over each element of the mapping\n",
    "    for i in range(0, dims[0]):\n",
    "        # second later of chain rule\n",
    "        outer_denom_1 = delta1[i]*np.linalg.norm(delta1)\n",
    "        outer_denom_2 = delta2[i]*np.linalg.norm(delta2)\n",
    "        for j in range(0, dims[1]):\n",
    "            # final layer of chain rule\n",
    "            frac_1 = (delta1[i]**2)*(s1[j]- s2[j])/outer_denom_1\n",
    "            frac_2 = (delta2[i]**2)*(s3[j] - s4[j])/outer_denom_2\n",
    "            output[i,j] = -(frac_2 - frac_1)*outest\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here is a toy example to demonstrate what the output of this proportionality prior may look. Instead of using\n",
    "a 10x10 image though, we will use a 2x2 image to make the answer easier to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame image of coords: [1 2]\n",
      "[0 1 1 1]\n",
      "[[0 1]\n",
      " [1 1]]\n",
      "frame image of coords: [2 3]\n",
      "[1 0 1 1]\n",
      "[[1 0]\n",
      " [1 1]]\n",
      "frame image of coords: [2 1]\n",
      "[1 1 0 1]\n",
      "[[1 1]\n",
      " [0 1]]\n",
      "frame image of coords: [2 4]\n",
      "[1.  1.  1.  0.1]\n",
      "[[1.  1. ]\n",
      " [1.  0.1]]\n",
      "mapping: [[0 1 2 3]\n",
      " [4 5 6 7]]\n",
      "[[-0.30765568  0.          1.10747884 -0.71984084]\n",
      " [-0.30765568  0.          0.65043703 -0.30850322]]\n"
     ]
    }
   ],
   "source": [
    "image1 = np.array([0, 1, 1, 1]).T  # (0,0)\n",
    "image2 = np.array([1, 0, 1, 1]).T  # (1,0)\n",
    "image3 = np.array([1, 1, 0, 1]).T  # (0,0)\n",
    "image4 = np.array([1, 1, 1, 0.1]).T  # (1,0)\n",
    "frame1 = Data_Frame(np.array([1, 2]), 4, 1, image1)\n",
    "frame2 = Data_Frame(np.array([2, 3]), 4, 2, image2)\n",
    "frame3 = Data_Frame(np.array([2, 1]), 4, 1, image3)\n",
    "frame4 = Data_Frame(np.array([2, 4]), 4, 2, image4)\n",
    "mapping = np.arange(8).reshape((2, 4))\n",
    "\n",
    "batch = [frame1, frame2, frame3, frame4]\n",
    "\n",
    "for f in batch:\n",
    "    print(\"frame image of coords: {}\\n{}\".format(f.coords, f.image))\n",
    "    print(f.image.reshape((2,2)))\n",
    "\n",
    "print(\"mapping: {}\".format(mapping))\n",
    "print(proportionality_prior_gradient(batch, mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxXfa-K6-sM3"
   },
   "source": [
    "## Problem A: Temporal Cohesion Prior (15 points) <a id='partA'/>\n",
    "\n",
    "The temporal cohesion loss represents Newton's Law of Inertia by penalizing temporally adjacent timeframes for being too different from each other.\n",
    "\n",
    "In lecture, we defined the temporal cohesion loss as $E[||\\triangle s_t||^2]$, where $s_t$ is the state representation at time $t$.\n",
    "Use the function skeleton below to implement the gradient of the temporal cohesion loss.\n",
    "\n",
    "Hint: You will want to take the derivative of $E[||\\triangle s_t||^2]$ with respect to the `mapping` matrix, and implement that as part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0ZQ1yGFu-35S"
   },
   "outputs": [],
   "source": [
    "def temporal_cohesion_gradient(batch, mapping):\n",
    "    \"\"\"\n",
    "    :param batch an array of Data_Frame objects\n",
    "    :param mapping the current matrix that maps flattened image vectors to coordinates\n",
    "    :return the gradient of the temporal cohesion loss (should be the same dimensions as the mapping matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    time_steps = len(batch)\n",
    "    total_loss_grad = np.zeros(mapping.shape)\n",
    "    for i in range(0,time_steps-1):\n",
    "        loss_grad = temporal_loss_gradient(batch[i], batch[i+1], mapping)\n",
    "        total_loss_grad += loss_grad.reshape(mapping.shape)\n",
    "\n",
    "    return total_loss_grad/(time_steps-1)\n",
    "\n",
    "def temporal_loss_gradient(init_state, next_state, mapping):\n",
    "    dims = mapping.shape\n",
    "    output = np.zeros(dims)\n",
    "    for i in range(0, dims[0]):\n",
    "        outside_comp = mapping[i,:]@init_state.image - mapping[i,:]@next_state.image\n",
    "        for j in range(0, dims[1]):\n",
    "            output[i,j] = 2*outside_comp*(init_state.image[j] - next_state.image[j])\n",
    "    return output\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GfASWz6MmkB",
    "outputId": "e611466b-f5f4-4371-df72-bafac744de74",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checker.temporal_sol_check(temporal_cohesion_gradient)\n",
    "checker.test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W36sTMP_Ubd"
   },
   "source": [
    "## Problem B: Causality Prior (15 points) <a id='partB'/>\n",
    "\n",
    "The causality prior loss represents Newton's Third Law of equal and opposite reactions by penalizing same actions with different rewards having similar state representations.\n",
    "\n",
    "In lecture, we defined the proportionality prior loss between two timesteps $t_1$ and $t_2$ as $E[e^{-|| s_{t_2} -  s_{t_1}||} | a_{t_1} = a_{t_2} \\wedge r_{t_1} \\neq r_{t_2}]$.\n",
    "Use the function skeleton below to implement the gradient of the causality prior loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5xdK-ta9_YiH"
   },
   "outputs": [],
   "source": [
    "def causality_prior_gradient(batch, mapping):\n",
    "    \"\"\"\n",
    "    :param batch an array of Data_Frame objects\n",
    "    :param mapping the current matrix that maps flattened image vectors to coordinates\n",
    "    :return the gradient of the causality prior loss (should be the same dimensions as the mapping matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    time_steps = len(batch)\n",
    "    total_loss_grad = np.zeros(mapping.shape)\n",
    "    pairings = 0\n",
    "    for i in range(0, time_steps - 1):\n",
    "\n",
    "        for j in range(i + 1, time_steps - 1):\n",
    "            pairings += 1\n",
    "            if batch[i].action == batch[j].action and batch[i].reward != batch[j].reward:\n",
    "                loss_grad = causal_loss_gradient(batch[i].image, batch[j].image,\n",
    "                                                mapping)\n",
    "                total_loss_grad += loss_grad.reshape(mapping.shape)\n",
    "\n",
    "    return total_loss_grad / pairings\n",
    "\n",
    "def causal_loss_gradient(s1,s2,mapping):\n",
    "    dims = mapping.shape\n",
    "    output = np.zeros(dims)\n",
    "\n",
    "    delta = mapping@s1 - mapping@s2\n",
    "    delta_norm = np.linalg.norm(delta)\n",
    "    for i in range(0,dims[0]):\n",
    "        denom = 2*delta[i]*delta_norm\n",
    "        for j in range(0,dims[1]):\n",
    "            numer = delta[i]*np.exp(-delta_norm) * (s1[j] - s2[j])*delta[i]*2\n",
    "            output[i,j] = -numer/denom\n",
    "    return output\n",
    "\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checker.causal_sol_check(causality_prior_gradient)\n",
    "checker.test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem C: Repeatability Prior (15 points) <a id='partC'/>\n",
    "\n",
    "The repeatability prior loss represents Newton's Third Law of equal and opposite reactions by penalizing same actions taken from similar positions for transitioning to different positions.\n",
    "\n",
    "In lecture, we defined the proportionality prior loss between two timesteps $t_1$ and $t_2$ as $E[e^{-|| s_{t_2} -  s_{t_1}||}||\\triangle s_{t_2} - \\triangle s_{t_1}||^2  | a_{t_1} = a_{t_2}]$.\n",
    "Use the function skeleton below to implement the gradient of the repeatability prior loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def repeatability_prior_gradient(batch, mapping):\n",
    "    \"\"\"\n",
    "    :param batch an array of Data_Frame objects\n",
    "    :param mapping the current matrix that maps flattened image vectors to coordinates\n",
    "    :return the gradient of the repeatability prior loss (should be the same dimensions as the mapping matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    time_steps = len(batch)\n",
    "    total_loss_grad = np.zeros(mapping.shape)\n",
    "    pairings = 0\n",
    "    for i in range(0, time_steps - 1):\n",
    "        for j in range(i + 1, time_steps - 1):\n",
    "            if batch[i].action == batch[j].action:\n",
    "                pairings += 1\n",
    "                loss_grad = proportional_loss_gradient(batch[i].image, batch[i + 1].image, batch[j].image,\n",
    "                                                       batch[j + 1].image, mapping)\n",
    "                total_loss_grad += loss_grad.reshape(mapping.shape)\n",
    "\n",
    "    return total_loss_grad / pairings\n",
    "\n",
    "\n",
    "def repeatability_loss_gradient(s1, s2, s3, s4, mapping):\n",
    "    \"\"\"\n",
    "    :param batch:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dims = mapping.shape\n",
    "    output = np.zeros(dims)\n",
    "\n",
    "    delta1 = (mapping@s2 - mapping@s1)\n",
    "    delta2 = (mapping@s4 - mapping@s3)\n",
    "    base_priorish_loss = np.linalg.norm(delta2 - delta1)**2\n",
    "    base_causal_loss = np.exp(-np.linalg.norm(mapping@s3 - mapping@s1))\n",
    "    causal_losses = causal_loss_gradient(s1, s3, mapping)\n",
    "\n",
    "    for i in range(0,dims[0]):\n",
    "        outer = delta2[i] - delta1[i]\n",
    "        for j in range(0,dims[1]):\n",
    "            inner = s4[j] - s3[j] - s2[j] + s1[j]\n",
    "            output[i,j] = 2*inner*outer*base_causal_loss + causal_losses[i,j]*base_priorish_loss\n",
    "    return output\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checker.repeatability_sol_check(repeatability_prior_gradient)\n",
    "checker.test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE!! Great Work!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
